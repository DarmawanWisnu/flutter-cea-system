{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CEA System - ML Model Training with Visualization\n",
                "\n",
                "This notebook trains a Random Forest model for the CEA hydroponic system.\n",
                "\n",
                "## **Rull-based Logic**\n",
                "\n",
                "The training data uses a **priority system** to prevent conflicting actions:\n",
                "\n",
                "1. **Priority 1:** Critical water level (< 1.2) → Refill ONLY\n",
                "2. **Priority 2:** High PPM (> 840) → Dilute (if water < 2.5)\n",
                "3. **Priority 3:** pH out of range → Adjust pH\n",
                "4. **Priority 4:** Low PPM (< 560) → Add nutrient\n",
                "5. **Priority 5:** Micro-adjustments for fine-tuning\n",
                "\n",
                "This ensures the ML model learns **chemically efficient** actions without conflicts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install pandas numpy scikit-learn joblib matplotlib seaborn -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Upload BOTH CSV Files\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"Upload training_telemetry.csv and training_actuator_event.csv\")\n",
                "print(\"(Select BOTH files when the upload dialog appears)\\n\")\n",
                "\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Detect which file is which\n",
                "telemetry_file = None\n",
                "actuator_file = None\n",
                "\n",
                "for filename in uploaded.keys():\n",
                "    if 'telemetry' in filename.lower():\n",
                "        telemetry_file = filename\n",
                "    elif 'actuator' in filename.lower():\n",
                "        actuator_file = filename\n",
                "\n",
                "if not telemetry_file or not actuator_file:\n",
                "    raise ValueError(\"Please upload BOTH files: training_telemetry.csv and training_actuator_event.csv\")\n",
                "\n",
                "print(f\"✅ Telemetry: {telemetry_file}\")\n",
                "print(f\"✅ Actuator:  {actuator_file}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load and Merge Data\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"Loading datasets...\")\n",
                "telemetry_df = pd.read_csv(telemetry_file)\n",
                "actuator_df = pd.read_csv(actuator_file)\n",
                "\n",
                "print(f\"  Telemetry rows: {len(telemetry_df):,}\")\n",
                "print(f\"  Actuator rows:  {len(actuator_df):,}\")\n",
                "\n",
                "# Merge on id, deviceId, ingestTime\n",
                "df = pd.merge(\n",
                "    telemetry_df,\n",
                "    actuator_df,\n",
                "    on=['deviceId', 'ingestTime'],\n",
                "    how='inner',\n",
                "    suffixes=('_telemetry', '_actuator')\n",
                ")\n",
                "\n",
                "print(f\"\\n✅ Merged dataset: {len(df):,} rows\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Data Analysis & Visualization\n",
                "print(\"Dataset Statistics:\\n\")\n",
                "print(df.describe())\n",
                "\n",
                "# Action distribution\n",
                "total = len(df)\n",
                "actions = (df['phUp'] > 0) | (df['phDown'] > 0) | (df['nutrientAdd'] > 0) | (df['refill'] > 0)\n",
                "action_count = actions.sum()\n",
                "\n",
                "print(f\"\\n Action Distribution:\")\n",
                "print(f\"  Total events: {total:,}\")\n",
                "print(f\"  Action events: {action_count:,} ({action_count/total*100:.1f}%)\")\n",
                "print(f\"  No action: {total-action_count:,} ({(total-action_count)/total*100:.1f}%)\")\n",
                "\n",
                "# Check for conflicting actions (should be ZERO with priority system)\n",
                "conflicts = (\n",
                "    ((df['phUp'] > 0) & (df['refill'] > 0)) |\n",
                "    ((df['phDown'] > 0) & (df['refill'] > 0)) |\n",
                "    ((df['nutrientAdd'] > 0) & (df['refill'] > 0)) |\n",
                "    ((df['phUp'] > 0) & (df['nutrientAdd'] > 0)) |\n",
                "    ((df['phDown'] > 0) & (df['nutrientAdd'] > 0))\n",
                ").sum()\n",
                "\n",
                "print(f\"\\n Conflicting Actions: {conflicts} ({conflicts/total*100:.2f}%)\")\n",
                "if conflicts == 0:\n",
                "    print(\"✅ No conflicts! Priority system working correctly.\")\n",
                "else:\n",
                "    print(f\"WARNING: {conflicts} events have conflicting actions!\")\n",
                "\n",
                "# Visualize action distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Pie chart\n",
                "axes[0].pie([action_count, total-action_count], \n",
                "           labels=['Action', 'No Action'], \n",
                "           autopct='%1.1f%%',\n",
                "           colors=['#ff6b6b', '#51cf66'])\n",
                "axes[0].set_title('Action vs No-Action Events', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Bar chart for each actuator\n",
                "actuator_counts = {\n",
                "    'phUp': (df['phUp'] > 0).sum(),\n",
                "    'phDown': (df['phDown'] > 0).sum(),\n",
                "    'nutrientAdd': (df['nutrientAdd'] > 0).sum(),\n",
                "    'refill': (df['refill'] > 0).sum()\n",
                "}\n",
                "axes[1].bar(actuator_counts.keys(), actuator_counts.values(), color=['#4dabf7', '#ff8787', '#fcc419', '#51cf66'])\n",
                "axes[1].set_title('Actuator Activation Frequency', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Feature distributions\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
                "features = ['ppm', 'ph', 'tempC', 'humidity', 'waterLevel', 'waterTemp']\n",
                "\n",
                "for idx, feature in enumerate(features):\n",
                "    row = idx // 3\n",
                "    col = idx % 3\n",
                "    axes[row, col].hist(df[feature], bins=50, color='skyblue', edgecolor='black')\n",
                "    axes[row, col].set_title(f'{feature} Distribution', fontweight='bold')\n",
                "    axes[row, col].set_xlabel(feature)\n",
                "    axes[row, col].set_ylabel('Frequency')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Prepare Training Data\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "FEATURES = ['ppm', 'ph', 'tempC', 'humidity', 'waterTemp', 'waterLevel']\n",
                "TARGETS = ['phUp', 'phDown', 'nutrientAdd', 'refill']\n",
                "\n",
                "X = df[FEATURES].copy()\n",
                "y = df[TARGETS].copy()\n",
                "\n",
                "# Handle missing values\n",
                "X = X.fillna(method='ffill').fillna(0.0)\n",
                "y = y.fillna(0)\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"✅ Training set: {len(X_train):,} samples\")\n",
                "print(f\"✅ Test set:     {len(X_test):,} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Train Model\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.multioutput import MultiOutputRegressor\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import joblib\n",
                "import datetime\n",
                "import json\n",
                "\n",
                "print(\"Training Random Forest Model...\\n\")\n",
                "\n",
                "# Create model\n",
                "base_model = RandomForestRegressor(\n",
                "    n_estimators=100,\n",
                "    max_depth=20,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    n_jobs=-1,\n",
                "    random_state=42,\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "model = MultiOutputRegressor(base_model)\n",
                "\n",
                "# Train\n",
                "model.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(\"\\n✅ Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Evaluate Model\n",
                "print(\"Evaluating model...\\n\")\n",
                "\n",
                "# Predictions\n",
                "y_train_pred = model.predict(X_train_scaled)\n",
                "y_test_pred = model.predict(X_test_scaled)\n",
                "\n",
                "# Calculate metrics for each target\n",
                "metrics = {}\n",
                "for i, target in enumerate(TARGETS):\n",
                "    train_mae = mean_absolute_error(y_train.iloc[:, i], y_train_pred[:, i])\n",
                "    test_mae = mean_absolute_error(y_test.iloc[:, i], y_test_pred[:, i])\n",
                "    train_rmse = np.sqrt(mean_squared_error(y_train.iloc[:, i], y_train_pred[:, i]))\n",
                "    test_rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], y_test_pred[:, i]))\n",
                "    train_r2 = r2_score(y_train.iloc[:, i], y_train_pred[:, i])\n",
                "    test_r2 = r2_score(y_test.iloc[:, i], y_test_pred[:, i])\n",
                "    \n",
                "    metrics[target] = {\n",
                "        'train_mae': train_mae,\n",
                "        'test_mae': test_mae,\n",
                "        'train_rmse': train_rmse,\n",
                "        'test_rmse': test_rmse,\n",
                "        'train_r2': train_r2,\n",
                "        'test_r2': test_r2\n",
                "    }\n",
                "    \n",
                "    print(f\"   {target.upper()}:\")\n",
                "    print(f\"   Train MAE:  {train_mae:.3f}  |  Test MAE:  {test_mae:.3f}\")\n",
                "    print(f\"   Train RMSE: {train_rmse:.3f}  |  Test RMSE: {test_rmse:.3f}\")\n",
                "    print(f\"   Train R²:   {train_r2:.3f}  |  Test R²:   {test_r2:.3f}\")\n",
                "    print()\n",
                "\n",
                "# Overall metrics\n",
                "overall_test_mae = mean_absolute_error(y_test, y_test_pred)\n",
                "overall_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
                "\n",
                "print(f\"   Overall Test Performance:\")\n",
                "print(f\"   MAE:  {overall_test_mae:.3f}\")\n",
                "print(f\"   RMSE: {overall_test_rmse:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Visualize Model Performance\n",
                "\n",
                "# Metrics comparison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "targets_list = list(TARGETS)\n",
                "train_maes = [metrics[t]['train_mae'] for t in targets_list]\n",
                "test_maes = [metrics[t]['test_mae'] for t in targets_list]\n",
                "train_r2s = [metrics[t]['train_r2'] for t in targets_list]\n",
                "test_r2s = [metrics[t]['test_r2'] for t in targets_list]\n",
                "\n",
                "# MAE comparison\n",
                "x = np.arange(len(targets_list))\n",
                "width = 0.35\n",
                "axes[0].bar(x - width/2, train_maes, width, label='Train', color='skyblue')\n",
                "axes[0].bar(x + width/2, test_maes, width, label='Test', color='salmon')\n",
                "axes[0].set_ylabel('MAE')\n",
                "axes[0].set_title('Mean Absolute Error', fontweight='bold')\n",
                "axes[0].set_xticks(x)\n",
                "axes[0].set_xticklabels(targets_list, rotation=45)\n",
                "axes[0].legend()\n",
                "axes[0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# R² comparison\n",
                "axes[1].bar(x - width/2, train_r2s, width, label='Train', color='lightgreen')\n",
                "axes[1].bar(x + width/2, test_r2s, width, label='Test', color='orange')\n",
                "axes[1].set_ylabel('R² Score')\n",
                "axes[1].set_title('R² Score (Higher is Better)', fontweight='bold')\n",
                "axes[1].set_xticks(x)\n",
                "axes[1].set_xticklabels(targets_list, rotation=45)\n",
                "axes[1].legend()\n",
                "axes[1].grid(axis='y', alpha=0.3)\n",
                "axes[1].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Good (0.8)')\n",
                "\n",
                "# Feature importance (average across all targets)\n",
                "importances = np.mean([est.feature_importances_ for est in model.estimators_], axis=0)\n",
                "axes[2].barh(FEATURES, importances, color='mediumpurple')\n",
                "axes[2].set_xlabel('Importance')\n",
                "axes[2].set_title('Feature Importance', fontweight='bold')\n",
                "axes[2].grid(axis='x', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Prediction vs Actual for each target\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for i, target in enumerate(TARGETS):\n",
                "    axes[i].scatter(y_test.iloc[:, i], y_test_pred[:, i], alpha=0.3, s=10)\n",
                "    axes[i].plot([y_test.iloc[:, i].min(), y_test.iloc[:, i].max()], \n",
                "                [y_test.iloc[:, i].min(), y_test.iloc[:, i].max()], \n",
                "                'r--', lw=2, label='Perfect Prediction')\n",
                "    axes[i].set_xlabel('Actual')\n",
                "    axes[i].set_ylabel('Predicted')\n",
                "    axes[i].set_title(f'{target.upper()} - Predicted vs Actual\\nR² = {metrics[target][\"test_r2\"]:.3f}', \n",
                "                     fontweight='bold')\n",
                "    axes[i].legend()\n",
                "    axes[i].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Save and Download Model\n",
                "ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
                "version = \"v\" + ts\n",
                "output_dir = version\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Save model and scaler\n",
                "joblib.dump(model, os.path.join(output_dir, \"model.pkl\"))\n",
                "joblib.dump(scaler, os.path.join(output_dir, \"scaler.pkl\"))\n",
                "\n",
                "# Save metadata\n",
                "metadata = {\n",
                "    \"version\": version,\n",
                "    \"timestamp\": ts,\n",
                "    \"training_samples\": len(X_train),\n",
                "    \"test_samples\": len(X_test),\n",
                "    \"features\": FEATURES,\n",
                "    \"targets\": TARGETS,\n",
                "    \"metrics\": {\n",
                "        target: {\n",
                "            \"test_mae\": float(metrics[target]['test_mae']),\n",
                "            \"test_rmse\": float(metrics[target]['test_rmse']),\n",
                "            \"test_r2\": float(metrics[target]['test_r2'])\n",
                "        } for target in TARGETS\n",
                "    },\n",
                "    \"overall_test_mae\": float(overall_test_mae),\n",
                "    \"overall_test_rmse\": float(overall_test_rmse),\n",
                "    \"logic_version\": \"priority_based_v1\",\n",
                "    \"conflicting_actions\": int(conflicts)\n",
                "}\n",
                "\n",
                "with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(f\"Model saved to: {output_dir}/\")\n",
                "print(f\"\\nFiles created:\")\n",
                "print(f\"  - model.pkl\")\n",
                "print(f\"  - scaler.pkl\")\n",
                "print(f\"  - metadata.json\")\n",
                "\n",
                "# Zip and download\n",
                "!zip -r {version}.zip {version}\n",
                "print(f\"\\n Downloading {version}.zip...\")\n",
                "files.download(f\"{version}.zip\")\n",
                "print(\"\\n Done! Extract the zip and copy to services/ml/model_registry/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
